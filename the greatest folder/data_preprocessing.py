# -*- coding: utf-8 -*-
"""Data Preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hKpvFqP-dMpr8Hq6TTczH3Ud-khaBgrI
"""

import os
import pandas as pd
import requests
files_to_break = ["QA/27-7-2022-Du thao Quy che TS.doc",
                  "QA/1533.VNU_Kế hoạch triển khai công tác tuyển sinh ĐHCQ năm 2024 của ĐHQGHN.pdf",
                  "QA/3626_21.10.2022. Quy chế đào tạo đại học tại ĐHQGHN (áp dụng từ khóa QH2022).docx",
                  "QA/4555 Quy dinh mo nganh va dieu chinh CTĐT tại ĐHQGHN.doc",
                  "QA/CV số 1957 Hướng dẫn TS năm 2024.pdf",
                  "QA/Du thao Quy che dao tao ThS 2022 V1.docx",
                  "QA/QHI_Đề án TS ĐHCQ năm 2024 (Điều chỉnh).doc",
                  "QA/Signed.Signed.Signed.Signed.Signed.Quyết định ban hành Quy chế tuyển sinh đại học chính quy tại ĐHQGHN (trình ký) (1).pdf",
                  "QA/Signed.Signed.Signed.Signed.Signed.Quyết định ban hành Quy chế tuyển sinh đại học chính quy tại ĐHQGHN (trình ký).docx",
                  "QA/V10_Dự thảo quy định VB CC CN -trình ký 15.4.2023.docx"]
# !pip install pytesseract pdf2image
# !apt-get install tesseract-ocr
# !apt-get install -y poppler-utils

def text_formatter(text:str) -> str:
  cleaned_text = text.replace("-\n"," ").strip()
  # More text formatting can go here
  return cleaned_text

import pytesseract
from pdf2image import convert_from_path
from tqdm.auto import tqdm

# 8 - 10 it/s
# 8 - 9 mins on GPU

def text_formatter(text: str) -> str:
    """Format and clean extracted text."""
    return text.replace("-\n", " ").strip()

def open_and_read_image_pdf(pdf_path: str) -> list[dict]:
    """Convert PDF pages to images and extract text using OCR."""
    images = convert_from_path(pdf_path)
    page_from = pdf_path.split("/")[-1].split(".pdf")[0]
    pages_and_texts = []

    for i, image in enumerate(tqdm(images)):
        text = pytesseract.image_to_string(image)
        text = text_formatter(text=text)
        pages_and_texts.append({
            "page_number": i + 1,
            "page_from": page_from,
            "page_char_count": len(text),
            "page_word_count": len(text.split(" ")),
            "page_sentence_count": len(text.split(". ")),
            "page_token_count": len(text) / 4,  # Assuming 1 token = ~4 characters
            "text": text
        })

    return pages_and_texts

def process_pdf_files(pdf_files: list[str]) -> list[dict]:
    """Process multiple PDF files."""
    all_pages_and_texts = []
    for pdf_path in pdf_files:
        if pdf_path.endswith(".pdf"):
            print(f"Processing PDF: {pdf_path}")
            pages_and_texts = open_and_read_image_pdf(pdf_path=pdf_path)
            all_pages_and_texts.extend(pages_and_texts)  # Combine all results
    return all_pages_and_texts

# Process all PDFs
all_pages_and_texts = process_pdf_files(files_to_break)

for page_data in all_pages_and_texts:
    if page_data['text'] is not str:
      print(page_data['text'])

all_pages_and_texts

def convert_floats_to_strings(data: dict) -> dict:
    """Converts all float values in the dictionary to strings."""
    for key, value in data.items():
        if isinstance(value, float):
            data[key] = str(value)
    return data
all_pages_and_texts = [
    {k: str(v) if isinstance(v, float) else v for k, v in d.items()}
    for d in all_pages_and_texts
]

all_pages_and_texts_df = pd.DataFrame(all_pages_and_texts)
all_pages_and_texts_df

# EXTRACT DOCX

from spire.doc import *
from spire.doc.common import *

# To store all extracted text
all_doc_texts = []

def text_formatter(text: str) -> str:
    """Format and clean extracted text."""
    text = text.replace("Evaluation Warning: The document was created with Spire.Doc for Python.", "")

    text = text.replace("\r", " ").replace("\n", " ")

    text = ' '.join(text.split())

    return text

# Function to process DOCX files and extract text
def open_and_read_docx(file_path: str) -> list[dict]:
    pages_and_texts = []
    docExtractor = Document()
    docExtractor.LoadFromFile(file_path)
    page_from = file_path.split("/")[-1].split(".docx")[0]

    # Get the text from the document
    text = docExtractor.GetText()
    text = text_formatter(text=text)

    # Since DOCX does not have distinct pages, we treat the entire document as one unit
    pages_and_texts.append({
        "page_number": 1,
        "page_from": page_from,
        "page_char_count": len(text),
        "page_word_count": len(text.split(" ")),
        "page_sentence_count": len(text.split(". ")),
        "page_token_count": len(text) / 4,  # Assuming 1 token = ~4 characters
        "text": text
    })

    docExtractor.Close()
    return pages_and_texts

# Processing multiple DOC/DOCX files
for doc in files_to_break:
    if doc.endswith(".docx") or doc.endswith(".doc"):
        all_doc_texts.extend(open_and_read_docx(doc))

for doc_text in all_doc_texts:
    print(doc_text)

all_doc_texts[:1]

all_doc_texts_df = pd.DataFrame(all_doc_texts)
all_doc_texts_df[1:]

all_pages_and_texts_df

from sentence_transformers import SentenceTransformer
sentences = ["Cô giáo đang ăn kem", "Chị gái đang thử món thịt dê"]

model = SentenceTransformer('keepitreal/vietnamese-sbert')
embeddings = model.encode(sentences)
print(embeddings)

from sentence_transformers import SentenceTransformer
sentences = all_doc_texts_df['text'].astype(str).tolist() + all_pages_and_texts_df['text'].astype(str).tolist()

model = SentenceTransformer('keepitreal/vietnamese-sbert')
embeddings = model.encode(sentences)
print(embeddings)

print(embeddings.shape)

doc_text = all_doc_texts_df['text']
embeddings = model.encode(doc_text)
all_doc_texts_df['text_embeddings'] = embeddings.tolist()

all_doc_texts_df

pdf_text = all_pages_and_texts_df['text']
embeddings = model.encode(pdf_text)
all_pages_and_texts_df['text_embeddings'] = embeddings.tolist()

all_pages_and_texts_df

from elasticsearch import Elasticsearch

# Change the scheme to http if you are not using SSL/TLS
es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])

for index, row in all_pages_and_texts_df.iterrows():
    document = {
        "page_number": row["page_number"],
        "page_from": row["page_from"],
        "page_char_count": row["page_char_count"],
        "page_word_count": row["page_word_count"],
        "page_sentence_count": row["page_sentence_count"],
        "page_token_count": row["page_token_count"],
        "text": row["text"],
        "text_embeddings": row["text_embeddings"]
    }

    es.index(index="pdf_embeddings", id=index, document=document)

# Query the index to check the stored documents
response = es.search(index="pdf_embeddings", body={
    "query": {
        "match_all": {}
    }
})

print(response)

documents_to_break = [
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Quy chế đào tạo ĐH, SĐH/4555 Quy dinh mo nganh va dieu chinh CTĐT tại ĐHQGHN.doc",
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Quy chế đào tạo ĐH, SĐH/V10_Dự thảo quy định VB CC CN -trình ký 15.4.2023.docx",
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Quy chế đào tạo ĐH, SĐH/3626_21.10.2022. Quy chế đào tạo đại học tại ĐHQGHN (áp dụng từ khóa QH2022).docx",
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Quy chế đào tạo ĐH, SĐH/Du thao Quy che dao tao ThS 2022 V1.docx",
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Quy chế đào tạo ĐH, SĐH/27-7-2022-Du thao Quy che TS.doc",
    "/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Tuyển sinh /QHI_Đề án TS ĐHCQ năm 2024 (Điều chỉnh).docx",
]

# # Table breaker 123
# import re

# splitter = re.compile(r"\s\s+")

# ability_test = {}
# for doc in documents_to_break:
#   with open(f"{doc}","r", encoding='utf-8', errors = 'replace') as f:
#     headers = splitter.split(next(f).strip())
#     for header in headers:
#       ability_test[header] = []
#     for line in f:
#       values = splitter.split(line.strip())
#       for header, value in zip(headers, values):
#         ability_test[header].append(value)
#   for header, values in ability_test.items():
#     print(f"{header}: {values[:5]}")

# # Table breaker 123
# import re

# splitter = re.compile(r"\s\s+")
# with open(f"/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Tuyển sinh /QHI_Đề án TS ĐHCQ năm 2024 (Điều chỉnh).docx","r", encoding='utf-8', errors = 'ignore') as f:
#   headers = splitter.split(next(f).strip())
#   for header in headers:
#     ability_test[header] = []
#   for line in f:
#     values = splitter.split(line.strip())
#     for header, value in zip(headers, values):
#       ability_test[header].append(values)
#     print(f"{header}: {values[:5]}")

# !pip install python-docx

# import docx

# doc = docx.Document("/content/drive/My Drive/Colab Notebooks/Dữ liệu nguồn về đào tạo (dùng tạo QA)/Tuyển sinh /QHI_Đề án TS ĐHCQ năm 2024 (Điều chỉnh).docx")
# all_tables = []

# for table in doc.tables:
#     table_data = []
#     for row in table.rows:
#         row_data = [cell.text.strip() for cell in row.cells]
#         table_data.append(row_data)
#     all_tables.append(table_data)

# for i, table in enumerate(all_tables, start=1):
#     print(f"\nTable {i}:")
#     for row in table:
#         print(row)

# print(len(all_tables[12]))

# import pandas as pd
# valid_tables_dict = {}
# j = 0
# for i in range(len(all_tables)):
#   if len(all_tables[i]) > 1:
#     df = pd.DataFrame(all_tables[i])
#     j+=1
#     valid_tables_dict[f"table_{j}_df"] = df
# valid_tables_dict['table_2_df']

# table_two_df = pd.DataFrame(all_tables[1][2:], columns = pd.MultiIndex.from_arrays([all_tables[1][0], all_tables[1][1]]))
# table_two_df.head()

# from sentence_transformers import SentenceTransformer

# embedding_model = SentenceTransformer("dangvantuan/vietnamese-embedding")

# # embedding_model.encode(valid_tables_dict['table_2_df'])
# valid_tables_dict['table_2_df'][0]

